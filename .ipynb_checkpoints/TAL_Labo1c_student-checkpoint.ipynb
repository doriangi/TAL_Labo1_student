{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAL Labo 1c : mêmes opérations sur une page web en anglais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Objectif** \n",
    "\n",
    "Dans cette troisième partie du Labo1, vous allez refaire une partie des traitements de la partie 1b, mais cette fois-ci sur une page web, spécifiquement une page Wikipedia en anglais (suggestion : \"Switzerland\").  L'objectif est de réviser les principales commandes apprises, et de traiter le format HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\oscar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import os\n",
    "from urllib import request\n",
    "import matplotlib.pyplot \n",
    "%matplotlib inline\n",
    "#nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "S'inspirer du [chapitre 3 du livre NLTK](http://www.nltk.org/book/ch03.html) pour faire une requête et récupérer le contenu de la page indiquée dans `url2`.  Quelle est la longueur de la chaîne de caractères obtenue ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "938695"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url2 = \"https://en.wikipedia.org/wiki/Switzerland\" \n",
    "response = request.urlopen(url2)\n",
    "html2 = response.read().decode('utf-8')\n",
    "len(html2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utilserons l'outil `BeautifulSoup` disponible sous forme de module Python pour extraire tout le texte de la page HTML.  Si le contenu de la page est stocké dans `html2`, nous extrayons le texte dans `raw2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161615"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw2 = BeautifulSoup(html2).get_text()\n",
    "len(raw2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Réutilisez la méthode de la partie 1b pour enlever le début et la fin de la chaîne `raw2`, car ils contiennent du texte qui n'est pas pertinent pour notre analyse (\"ne parle pas de la Suisse\").  Quelle est la longueur du résultat ? Afficher aussi ses 100 premiers et 100 derniers caractères (p.ex. `raw2trimmed`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. The most widespread varieties are the Chasselas (called Fendant in Valais) and Pinot Noir. Merlot '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "cropped = raw2[raw2.find(\"Switzerland, officially the Swiss Confederation\"):raw2.rfind(\"is the main variety produced in Ticino\")]\n",
    "stripped = cropped.replace('\\r', '').replace('\\n', ' ')\n",
    "raw2trimmed = re.sub(r'\\s+', ' ', stripped)\n",
    "raw2trimmed[:100] \n",
    "raw2trimmed[(len(raw2trimmed)-100):len(raw2trimmed)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effectuer la segmentation en phrases, puis tokeniser chaque phrase.  Ecrivez le résultat (une phrase par ligne, espaces entre *tokens*) dans un fichier `sample_web_page.txt` et inspectez-le avec un éditeur de texte.  Observez-vous des imperfections ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"sample_web_page.txt\"\n",
    "# Pour un fichier local : chemin relatif par rapport au notebook\n",
    "# Pour Google Colab, p.ex.: /content/gdrive/My Drive/sample_web_page.txt\n",
    "if os.path.exists(filename): \n",
    "    os.remove(filename)\n",
    "sentences = nltk.tokenize.sent_tokenize(raw2trimmed, language='french')\n",
    "\n",
    "with open(filename, 'a', encoding='utf8') as fd:\n",
    "    # Please write your Python code below and execute it.\n",
    "    for sentence in sentences : \n",
    "        fd.write(sentence+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Effectuer maintenant la tokenisation de cette page **sans** faire de segmentation en phrases.  Stockez le résultat dans une variable (p.ex. `words2`) sans écrire de fichier.  Combien de tokens possède ce texte ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16838"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "words2 = nltk.tokenize.word_tokenize(raw2trimmed)\n",
    "len(words2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Créez un objet de type nltk.Text à partir de la liste de *tokens* `words2`.  Appliquez-lui à titre d'exemple les méthodes `concordance`, `similar` et `collocations`. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 25 of 215 matches:\n",
      " Switzerland , officially the Swiss Confederat\n",
      "a and Liechtenstein to the east . Switzerland is geographically divided among t\n",
      "ürich , Geneva and Basel . [ 14 ] Switzerland originates from the Old Swiss Con\n",
      "the Peace of Westphalia in 1648 . Switzerland has maintained a policy of armed \n",
      "vement in peace building . [ 15 ] Switzerland is the birthplace of the Red Cros\n",
      "le market and the Schengen Area . Switzerland is a federal republic composed of\n",
      "based in Bern . [ a ] [ 2 ] [ 1 ] Switzerland is one of the world 's most devel\n",
      " GDP ) per capita . [ 17 ] [ 18 ] Switzerland ranks first in the Human Developm\n",
      "icity , and religion , leading to Switzerland being described as a Willensnatio\n",
      "Due to its linguistic diversity , Switzerland is known by multiple native names\n",
      " Etymology Main article : Name of Switzerland The English name Switzerland is a\n",
      "e of Switzerland The English name Switzerland is a portmanteau of Switzer , an \n",
      " 14th century . The data code for Switzerland , CH , is derived from Latin Conf\n",
      "History Main article : History of Switzerland The state of Switzerland took its\n",
      "story of Switzerland The state of Switzerland took its present form with the ad\n",
      "ss Federal Constitution in 1848 . Switzerland 's precursors established a defen\n",
      " Main articles : Early history of Switzerland and Switzerland in the Roman era \n",
      " Early history of Switzerland and Switzerland in the Roman era The oldest trace\n",
      "st traces of hominid existence in Switzerland date to about 150,000 years ago .\n",
      "dest known farming settlements in Switzerland , which were found at Gächlingen \n",
      "important archaeological sites in Switzerland . [ 35 ] The earliest known tribe\n",
      "e . The eastern portion of modern Switzerland was integrated into the Roman pro\n",
      "of the Rhine transformed today 's Switzerland into a frontier land of the Empir\n",
      " the western extent of modern-day Switzerland was part of the territory of the \n",
      " , forming Alemannia . Modern-day Switzerland was then divided between the king\n"
     ]
    }
   ],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "text = nltk.Text(words2)\n",
    "text.concordance(\"Switzerland\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the and basel europe it to population geneva countries gdp ad in\n",
      "nations with bern while under mediation september gaeta\n"
     ]
    }
   ],
   "source": [
    "text.similar(\"Switzerland\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "citation needed; Main article; United Nations; Main articles; St.\n",
      "Gallen; Federal Council; European Union; Swiss Plateau; Federal\n",
      "Assembly; direct democracy; per capita; See also; Red Cross; European\n",
      "countries; bilateral agreements; Holy Roman; Statistical Office;\n",
      "Catholic Church; Swiss Confederacy; Helvetic Republic\n"
     ]
    }
   ],
   "source": [
    "text.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Déterminez le vocabulaire de cette page (la liste des _types_) en convertissant la liste des _tokens_ en un `set`.  \n",
    "* Veuillez calculer ainsi le vocabulaire de votre texte.\n",
    "* Combien de mots différents a-t-il ?  (En incluant les ponctuations et tout autre symbole.) \n",
    "* Quels sont les mots qui ont plus de 15 lettres ?  (Ou une autre taille qui vous semble intéressante.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'set' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Please write your Python code below and execute it.\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words2 :\n\u001b[0;32m      4\u001b[0m     s\u001b[38;5;241m.\u001b[39madd(w)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'set' object is not callable"
     ]
    }
   ],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "s = set()\n",
    "for w in words2 :\n",
    "    s.add(w)\n",
    "print(len(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list({s for s in s if len(s) >= 15})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construisez un objet `FreqDist` avec les mots de cette page, en convertissant en minuscules tous les mots contenant seulement des lettres (utilisez la méthode `.isalpha()` de Python).  Quels sont les 30 mots les plus fréquents ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "from nltk.probability import FreqDist\n",
    "fdist = FreqDist()\n",
    "for w in words2:\n",
    "    if(w.isalpha()):\n",
    "        fdist[w.lower()] += 1\n",
    "fdist.pformat(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Affichez le graphique cumulatif du nombre d'occurrences des 70 mots les plus fréquents de votre texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize = (10,4))\n",
    "plt.gcf().subplots_adjust(bottom=0.15) # to avoid x-ticks cut-off\n",
    "fdist.plot(70, cumulative=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construisez une liste avec la longueur de chaque token du texte, créez un nouvel objet `FreqDist` à partir de cette liste, et affichez la distribution (non-cumulative) des nombres d'occurrences pour chaque longueur.  Qu'observez-vous ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write your Python code below and execute it.\n",
    "fdist2 = FreqDist()\n",
    "for w in words2:\n",
    "    if(w.isalpha()):\n",
    "        fdist2[w.lower()] += len(w)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "fig2 = plt.figure(figsize = (10,4))\n",
    "plt.gcf().subplots_adjust(bottom=0.15) # to avoid x-ticks cut-off\n",
    "fdist2.plot(70)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fin de la partie 1c du Labo1\n",
    "Veuillez nettoyer autant que possible ce _notebook_, exécutez une dernière fois toutes les cellules pour obtenir les résultats demandés, et enregistrez le _notebook_.  Puis ajoutez-le dans une archive _zip_ avec les _notebook_ des parties 1b et 1d, et soumettez l'archive individuellement sur Cyberlearn (_Laboratoire 1_). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
